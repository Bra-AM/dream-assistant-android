{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "817642ae3e4143fdb6025a5e97742027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85c2fa293add4b5aa87c50f0106834d2",
              "IPY_MODEL_d33e844bab774866ac162b93d6573b6d",
              "IPY_MODEL_ffd10af37bf84c39932b78af3684c62d"
            ],
            "layout": "IPY_MODEL_0d19667bcdb34fa3be306d2eac9d1a77"
          }
        },
        "85c2fa293add4b5aa87c50f0106834d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1763dff17f08489ea9954014ec738574",
            "placeholder": "​",
            "style": "IPY_MODEL_2bbbcacb22b74fa6a57564af8579ad8b",
            "value": "model.safetensors: 100%"
          }
        },
        "d33e844bab774866ac162b93d6573b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c380f79e98c34101b5afa89ca902e327",
            "max": 2224765114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85def3d9b373472c9448d248e0cd1fd5",
            "value": 2224765114
          }
        },
        "ffd10af37bf84c39932b78af3684c62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6afe7c31fa4258ae69cce90b7d46ab",
            "placeholder": "​",
            "style": "IPY_MODEL_ae80c74eae434058b69c5550c587e4dc",
            "value": " 2.22G/2.22G [00:07&lt;00:00, 92.0MB/s]"
          }
        },
        "0d19667bcdb34fa3be306d2eac9d1a77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1763dff17f08489ea9954014ec738574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bbbcacb22b74fa6a57564af8579ad8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c380f79e98c34101b5afa89ca902e327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85def3d9b373472c9448d248e0cd1fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f6afe7c31fa4258ae69cce90b7d46ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae80c74eae434058b69c5550c587e4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca7f1bf01604552886c6caa6ae1ddc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe16699a415c4da4bdf08f02edf6a00c",
              "IPY_MODEL_94d446b9b3d44132ae52dc63f552db92",
              "IPY_MODEL_16a01dc5812a44199d67d24e8a369798"
            ],
            "layout": "IPY_MODEL_7b146254defc45969553b80d6bf658c9"
          }
        },
        "fe16699a415c4da4bdf08f02edf6a00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_226d56f5699c453287ba5692c20347dc",
            "placeholder": "​",
            "style": "IPY_MODEL_ceff652dd2be485d8696f1e110500b82",
            "value": "generation_config.json: 100%"
          }
        },
        "94d446b9b3d44132ae52dc63f552db92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4683bc455d34f9d892fa69a9d213c27",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1e4616dd2ed4e8580724726c299f2e2",
            "value": 209
          }
        },
        "16a01dc5812a44199d67d24e8a369798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e05acb2759244dd9658d7b81cf77d15",
            "placeholder": "​",
            "style": "IPY_MODEL_492a41d4af17438091f7b8ec9672e4d5",
            "value": " 209/209 [00:00&lt;00:00, 25.2kB/s]"
          }
        },
        "7b146254defc45969553b80d6bf658c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "226d56f5699c453287ba5692c20347dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceff652dd2be485d8696f1e110500b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4683bc455d34f9d892fa69a9d213c27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e4616dd2ed4e8580724726c299f2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e05acb2759244dd9658d7b81cf77d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492a41d4af17438091f7b8ec9672e4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9776806258444e1b086c96628688f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b952e05f75bb4b46831a18f8889edee4",
              "IPY_MODEL_cef79e46009e418c877b558c64759dc4",
              "IPY_MODEL_1d91cca175134967a8d403036786d872"
            ],
            "layout": "IPY_MODEL_ebf1900fde5c4bbab56aa909b4420455"
          }
        },
        "b952e05f75bb4b46831a18f8889edee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ba661cc50ca46b1a56c7195dbbbb585",
            "placeholder": "​",
            "style": "IPY_MODEL_c3aa2ca9f1024433b735892b221b083a",
            "value": "tokenizer_config.json: "
          }
        },
        "cef79e46009e418c877b558c64759dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02e9ba753353433f9213b2b357c0ebdf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3d7815d7861494a84b5e531798330da",
            "value": 1
          }
        },
        "1d91cca175134967a8d403036786d872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef50a992d034a1bbd525850ecd72a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a7af8b5db84986bdf37013450716c0",
            "value": " 47.0k/? [00:00&lt;00:00, 5.55MB/s]"
          }
        },
        "ebf1900fde5c4bbab56aa909b4420455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba661cc50ca46b1a56c7195dbbbb585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3aa2ca9f1024433b735892b221b083a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e9ba753353433f9213b2b357c0ebdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e3d7815d7861494a84b5e531798330da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef50a992d034a1bbd525850ecd72a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a7af8b5db84986bdf37013450716c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f49ab7047dc144579586b35f596def19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cf6921b503442a599706ea29783452b",
              "IPY_MODEL_65c2a95d4a6a48cfaaa8777c0186d7fe",
              "IPY_MODEL_0c344a9392f049028a29b28e5c67e15e"
            ],
            "layout": "IPY_MODEL_5277eac071a14cfebd61896299fc3940"
          }
        },
        "8cf6921b503442a599706ea29783452b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7315890957b44ac09cbeab476501dca5",
            "placeholder": "​",
            "style": "IPY_MODEL_bc51470f1cf947109da2ed5b98f0cd34",
            "value": "tokenizer.model: 100%"
          }
        },
        "65c2a95d4a6a48cfaaa8777c0186d7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a09745664df54a71971a3a721c0856a9",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95facd14e4024be781289a7e126423cd",
            "value": 4241003
          }
        },
        "0c344a9392f049028a29b28e5c67e15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d1fe85d5e4481e9295582758a8de8d",
            "placeholder": "​",
            "style": "IPY_MODEL_01f9199e04d94c95ac92e0a1c561b776",
            "value": " 4.24M/4.24M [00:01&lt;00:00, 3.26MB/s]"
          }
        },
        "5277eac071a14cfebd61896299fc3940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7315890957b44ac09cbeab476501dca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc51470f1cf947109da2ed5b98f0cd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a09745664df54a71971a3a721c0856a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95facd14e4024be781289a7e126423cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7d1fe85d5e4481e9295582758a8de8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f9199e04d94c95ac92e0a1c561b776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "775dab5dcc42450d8c0faad91406c0d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4275cc2cc3f44206b4821d929d7c99ef",
              "IPY_MODEL_0641fe778da5452595556cea534c08a6",
              "IPY_MODEL_93e45970c3584bda94ec443855df1dca"
            ],
            "layout": "IPY_MODEL_5ec74d6be4784856bb8bf6d047ddd4ff"
          }
        },
        "4275cc2cc3f44206b4821d929d7c99ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b8a568578e4f228036cf9c8611c6c7",
            "placeholder": "​",
            "style": "IPY_MODEL_f89bee1f773c415fbdcaa0c630bd1f4e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0641fe778da5452595556cea534c08a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_763d57ab9cb54a8c835269c59e5c0b41",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f6979c15e63419aa545cd3a296b0a1b",
            "value": 636
          }
        },
        "93e45970c3584bda94ec443855df1dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2245039285f4771b1a1d4690304acbe",
            "placeholder": "​",
            "style": "IPY_MODEL_e23bcc80d51b4fe0b8e7cf1bd4c99260",
            "value": " 636/636 [00:00&lt;00:00, 85.2kB/s]"
          }
        },
        "5ec74d6be4784856bb8bf6d047ddd4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b8a568578e4f228036cf9c8611c6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f89bee1f773c415fbdcaa0c630bd1f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "763d57ab9cb54a8c835269c59e5c0b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6979c15e63419aa545cd3a296b0a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2245039285f4771b1a1d4690304acbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e23bcc80d51b4fe0b8e7cf1bd4c99260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da181855a5bb454d8e9596f6727848a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c06a268eddb7495990f516f25268459b",
              "IPY_MODEL_5cda367823954730b9e572f69c6cdc06",
              "IPY_MODEL_093bb4d088664e0b90e25f5f9fdb7c59"
            ],
            "layout": "IPY_MODEL_c9051b25e75d42e4b517338ca1d8a91f"
          }
        },
        "c06a268eddb7495990f516f25268459b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8741bcebc96f4bffb3acdd4217f4b539",
            "placeholder": "​",
            "style": "IPY_MODEL_a3373237f1f948d1b97ad8f8320e341e",
            "value": "tokenizer.json: 100%"
          }
        },
        "5cda367823954730b9e572f69c6cdc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e3a34629e14d42aee5b5ba969479c8",
            "max": 17525357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_692b0074a19843649a3f0de5de566b6f",
            "value": 17525357
          }
        },
        "093bb4d088664e0b90e25f5f9fdb7c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f10d906cfac94c959f77471acf7ea0b2",
            "placeholder": "​",
            "style": "IPY_MODEL_b965740bb3594f259175f15d18ada0b0",
            "value": " 17.5M/17.5M [00:01&lt;00:00, 14.7MB/s]"
          }
        },
        "c9051b25e75d42e4b517338ca1d8a91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8741bcebc96f4bffb3acdd4217f4b539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3373237f1f948d1b97ad8f8320e341e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39e3a34629e14d42aee5b5ba969479c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692b0074a19843649a3f0de5de566b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f10d906cfac94c959f77471acf7ea0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b965740bb3594f259175f15d18ada0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9807e3c1cc1343f29a5748b4b5b168ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_064d125d72c54422944ed3a1fdbcef25",
              "IPY_MODEL_3b5421291504459480f4f0627f5d620b",
              "IPY_MODEL_96f6ea8ddbc84d1c899bac85cfae5d05"
            ],
            "layout": "IPY_MODEL_184cc6c4249b48e0a285a896348ac6da"
          }
        },
        "064d125d72c54422944ed3a1fdbcef25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f99064902041a09c88c40393e39d15",
            "placeholder": "​",
            "style": "IPY_MODEL_81bcea521d1e4f37affa644d0bf414c0",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "3b5421291504459480f4f0627f5d620b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8663971cf12499b987f1b5c062f46cd",
            "max": 404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_673844a2677a4468873f69ac8eae2339",
            "value": 404
          }
        },
        "96f6ea8ddbc84d1c899bac85cfae5d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b00cdb1f58f4fdfa39426483e6255d8",
            "placeholder": "​",
            "style": "IPY_MODEL_01adb244158e452a9d2fd8a2401adfb3",
            "value": " 404/404 [00:00&lt;00:00, 9372.33 examples/s]"
          }
        },
        "184cc6c4249b48e0a285a896348ac6da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f99064902041a09c88c40393e39d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81bcea521d1e4f37affa644d0bf414c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8663971cf12499b987f1b5c062f46cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673844a2677a4468873f69ac8eae2339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b00cdb1f58f4fdfa39426483e6255d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01adb244158e452a9d2fd8a2401adfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBdUKIiRaYuU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth and dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "\n",
        "print(\"Unsloth installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Training Data"
      ],
      "metadata": {
        "id": "gw6Ychhwbapk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Upload training files:\")\n",
        "print(\"1. sister_speech_recognition.json\")\n",
        "print(\"2. sister_conversation.json\")\n",
        "\n",
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Upload training files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify uploads\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"✅ Uploaded: {filename} ({len(uploaded[filename])} bytes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "a3Na5wWRbD3e",
        "outputId": "913f4b37-d69e-405b-80fa-5b36a866c73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload training files:\n",
            "1. sister_speech_recognition.json\n",
            "2. sister_conversation.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84ac092f-c1dc-4c19-a4b2-8aeedc7d11e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-84ac092f-c1dc-4c19-a4b2-8aeedc7d11e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sister_conversation.json to sister_conversation.json\n",
            "Saving sister_speech_recognition.json to sister_speech_recognition.json\n",
            "✅ Uploaded: sister_conversation.json (79614 bytes)\n",
            "✅ Uploaded: sister_speech_recognition.json (116055 bytes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Prepare Data"
      ],
      "metadata": {
        "id": "xvc1SS3Pb0tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_training_data():\n",
        "    \"\"\"Load sister's training data\"\"\"\n",
        "    print(\" Loading sister's training data...\")\n",
        "\n",
        "    # Load datasets\n",
        "    with open('sister_speech_recognition.json', 'r', encoding='utf-8') as f:\n",
        "        speech_data = json.load(f)\n",
        "\n",
        "    with open('sister_conversation.json', 'r', encoding='utf-8') as f:\n",
        "        conversation_data = json.load(f)\n",
        "\n",
        "    print(f\" Speech samples: {len(speech_data)}\")\n",
        "    print(f\" Conversation samples: {len(conversation_data)}\")\n",
        "\n",
        "    return speech_data, conversation_data\n",
        "\n",
        "def format_for_training(speech_data, conversation_data):\n",
        "    \"\"\"Format data for Gemma 3n training\"\"\"\n",
        "    print(\" Formatting data for training...\")\n",
        "\n",
        "    training_texts = []\n",
        "\n",
        "    # Process conversation data (main training source)\n",
        "    for item in conversation_data:\n",
        "        conversations = item['conversations']\n",
        "\n",
        "        # Convert to Gemma 3n chat format\n",
        "        formatted_text = \"\"\n",
        "        for conv in conversations:\n",
        "            role = conv['role']\n",
        "            content = conv['content']\n",
        "\n",
        "            if role == 'user':\n",
        "                formatted_text += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
        "            elif role == 'assistant':\n",
        "                formatted_text += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
        "\n",
        "        training_texts.append({\"text\": formatted_text})\n",
        "\n",
        "    # Add speech patterns for better understanding\n",
        "    for item in speech_data:\n",
        "        conversations = item['conversations']\n",
        "        if len(conversations) >= 2:\n",
        "            assistant_content = conversations[1]['content']\n",
        "\n",
        "            # Create speech understanding sample\n",
        "            formatted_text = f\"<start_of_turn>user\\nMi hermana dice: {assistant_content}<end_of_turn>\\n\"\n",
        "            formatted_text += f\"<start_of_turn>model\\n¡Perfecto! Entendí: '{assistant_content}'. ¿En qué te puedo ayudar? 😊<end_of_turn>\\n\"\n",
        "\n",
        "            training_texts.append({\"text\": formatted_text})\n",
        "\n",
        "    print(f\" Total training samples: {len(training_texts)}\")\n",
        "    return training_texts\n",
        "\n",
        "# Load and format data\n",
        "speech_data, conversation_data = load_training_data()\n",
        "training_texts = format_for_training(speech_data, conversation_data)\n",
        "\n",
        "print(f\"\\n Training on {len(training_texts)} samples sister's voice!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Ppemhwbmvq",
        "outputId": "ff4993ce-d90c-40e1-f18c-54855e5e3e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading sister's training data...\n",
            " Speech samples: 202\n",
            " Conversation samples: 202\n",
            " Formatting data for training...\n",
            " Total training samples: 404\n",
            "\n",
            " Training on 404 samples sister's voice!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Gemma3n Model"
      ],
      "metadata": {
        "id": "pNfL76UHcO_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\" Loading Gemma 3n model with Unsloth...\")\n",
        "\n",
        "# Load Gemma model (using 2B for faster training)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Gemma-2-2b-it\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters for efficient fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(\"LoRA adapters added for efficient training!\")\n",
        "print(\"Ready to train users's personalized assistant!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566,
          "referenced_widgets": [
            "817642ae3e4143fdb6025a5e97742027",
            "85c2fa293add4b5aa87c50f0106834d2",
            "d33e844bab774866ac162b93d6573b6d",
            "ffd10af37bf84c39932b78af3684c62d",
            "0d19667bcdb34fa3be306d2eac9d1a77",
            "1763dff17f08489ea9954014ec738574",
            "2bbbcacb22b74fa6a57564af8579ad8b",
            "c380f79e98c34101b5afa89ca902e327",
            "85def3d9b373472c9448d248e0cd1fd5",
            "9f6afe7c31fa4258ae69cce90b7d46ab",
            "ae80c74eae434058b69c5550c587e4dc",
            "fca7f1bf01604552886c6caa6ae1ddc0",
            "fe16699a415c4da4bdf08f02edf6a00c",
            "94d446b9b3d44132ae52dc63f552db92",
            "16a01dc5812a44199d67d24e8a369798",
            "7b146254defc45969553b80d6bf658c9",
            "226d56f5699c453287ba5692c20347dc",
            "ceff652dd2be485d8696f1e110500b82",
            "b4683bc455d34f9d892fa69a9d213c27",
            "e1e4616dd2ed4e8580724726c299f2e2",
            "8e05acb2759244dd9658d7b81cf77d15",
            "492a41d4af17438091f7b8ec9672e4d5",
            "c9776806258444e1b086c96628688f3f",
            "b952e05f75bb4b46831a18f8889edee4",
            "cef79e46009e418c877b558c64759dc4",
            "1d91cca175134967a8d403036786d872",
            "ebf1900fde5c4bbab56aa909b4420455",
            "5ba661cc50ca46b1a56c7195dbbbb585",
            "c3aa2ca9f1024433b735892b221b083a",
            "02e9ba753353433f9213b2b357c0ebdf",
            "e3d7815d7861494a84b5e531798330da",
            "0ef50a992d034a1bbd525850ecd72a7d",
            "e5a7af8b5db84986bdf37013450716c0",
            "f49ab7047dc144579586b35f596def19",
            "8cf6921b503442a599706ea29783452b",
            "65c2a95d4a6a48cfaaa8777c0186d7fe",
            "0c344a9392f049028a29b28e5c67e15e",
            "5277eac071a14cfebd61896299fc3940",
            "7315890957b44ac09cbeab476501dca5",
            "bc51470f1cf947109da2ed5b98f0cd34",
            "a09745664df54a71971a3a721c0856a9",
            "95facd14e4024be781289a7e126423cd",
            "c7d1fe85d5e4481e9295582758a8de8d",
            "01f9199e04d94c95ac92e0a1c561b776",
            "775dab5dcc42450d8c0faad91406c0d1",
            "4275cc2cc3f44206b4821d929d7c99ef",
            "0641fe778da5452595556cea534c08a6",
            "93e45970c3584bda94ec443855df1dca",
            "5ec74d6be4784856bb8bf6d047ddd4ff",
            "10b8a568578e4f228036cf9c8611c6c7",
            "f89bee1f773c415fbdcaa0c630bd1f4e",
            "763d57ab9cb54a8c835269c59e5c0b41",
            "0f6979c15e63419aa545cd3a296b0a1b",
            "a2245039285f4771b1a1d4690304acbe",
            "e23bcc80d51b4fe0b8e7cf1bd4c99260",
            "da181855a5bb454d8e9596f6727848a7",
            "c06a268eddb7495990f516f25268459b",
            "5cda367823954730b9e572f69c6cdc06",
            "093bb4d088664e0b90e25f5f9fdb7c59",
            "c9051b25e75d42e4b517338ca1d8a91f",
            "8741bcebc96f4bffb3acdd4217f4b539",
            "a3373237f1f948d1b97ad8f8320e341e",
            "39e3a34629e14d42aee5b5ba969479c8",
            "692b0074a19843649a3f0de5de566b6f",
            "f10d906cfac94c959f77471acf7ea0b2",
            "b965740bb3594f259175f15d18ada0b0"
          ]
        },
        "id": "UNGLhOjtcGRa",
        "outputId": "d5d7732c-00c9-4155-facc-b126e9422f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Patching Xformers to fix some performance issues.\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.2.2+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.8 (you have 3.11.13)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            " Loading Gemma 3n model with Unsloth...\n",
            "==((====))==  Unsloth 2025.7.11: Fast Gemma2 patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "817642ae3e4143fdb6025a5e97742027"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/209 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca7f1bf01604552886c6caa6ae1ddc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9776806258444e1b086c96628688f3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f49ab7047dc144579586b35f596def19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "775dab5dcc42450d8c0faad91406c0d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da181855a5bb454d8e9596f6727848a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.7.11 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "LoRA adapters added for efficient training!\n",
            "Ready to train users's personalized assistant!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Training"
      ],
      "metadata": {
        "id": "G9qKR262cv71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(training_texts)\n",
        "print(f\" Dataset created with {len(dataset)} samples\")\n",
        "\n",
        "# Training arguments optimized for sister's voice\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sister_dream_assistant\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=150,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Training setup complete!\")\n",
        "print(\"Ready to create users's personalized Dream Assistant!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "9807e3c1cc1343f29a5748b4b5b168ae",
            "064d125d72c54422944ed3a1fdbcef25",
            "3b5421291504459480f4f0627f5d620b",
            "96f6ea8ddbc84d1c899bac85cfae5d05",
            "184cc6c4249b48e0a285a896348ac6da",
            "94f99064902041a09c88c40393e39d15",
            "81bcea521d1e4f37affa644d0bf414c0",
            "b8663971cf12499b987f1b5c062f46cd",
            "673844a2677a4468873f69ac8eae2339",
            "9b00cdb1f58f4fdfa39426483e6255d8",
            "01adb244158e452a9d2fd8a2401adfb3"
          ]
        },
        "id": "84gWX9Z3ccar",
        "outputId": "de9f392f-9826-4bdb-8ae0-5123d851073f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset created with 404 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/404 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9807e3c1cc1343f29a5748b4b5b168ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training setup complete!\n",
            "Ready to create users's personalized Dream Assistant!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "QQkLMz9PdFlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting fine-tuning for Sister's Dream Assistant...\")\n",
        "print(\"Creating a personalized model that understands her unique speech patterns!\")\n",
        "print(\"Training it to be her best friend and business mentor!\")\n",
        "print(\"\\nThis will take about 15-20 minutes...\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n TRAINING COMPLETE!\")\n",
        "print(\"You now have your own personalized Gemma 3n model!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YFmSBxCCc9yR",
        "outputId": "d308f458-0a11-4193-97ca-8e50ca0c5d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning for Sister's Dream Assistant...\n",
            "Creating a personalized model that understands her unique speech patterns!\n",
            "Training it to be her best friend and business mentor!\n",
            "\n",
            "This will take about 15-20 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 404 | Num Epochs = 3 | Total steps = 150\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 20,766,720 of 2,635,108,608 (0.79% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 03:40, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.638700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>6.687600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.062300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.476300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>4.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.825400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.663300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.417800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.201800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.182700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.416500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.531500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.563700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.345500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.204100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.064500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.998100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.941200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.078600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.773900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.804300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.634300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.796600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.695600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.779100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.511100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.531600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.435800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.393200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.396600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.412200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.548500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.457300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.301700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.343300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.536500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.243200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.327700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.256800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.259900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.312100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.223400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.463800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.373100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.284900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.257300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.276200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.214100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.308900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.305900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.324300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.243800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.237400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.233200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.294100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.391600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.242600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.246500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.191700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.202100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.303300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.236100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.241700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.310500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.220500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.215200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.246300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.257500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.238300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.203700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.209000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.260500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.213000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.218300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.211700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.190400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.190400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.224900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.205100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.223100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.226400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.220500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.205800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.225200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.200900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.228400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.186300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.189000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.235800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.201100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.199100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.239800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.219600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.170800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.221100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.182600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.189900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.190200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TRAINING COMPLETE!\n",
            "You now have your own personalized Gemma 3n model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "-1uMTRmze52Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Testing users's personalized Dream Assistant...\")\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Testing with realistic imputs\n",
        "test_inputs = [\n",
        "    \"Hola, ¿cómo estás?\",\n",
        "    \"Ayúdame con mi negocio de la plataforma\",\n",
        "    \"Quiero enviar un mensaje a mi mamá que diga hola\",\n",
        "    \"Me siento un poco triste hoy\",\n",
        "    \"Abre YouTube para grabar un video sobre mi proyecto\",\n",
        "    \"Necesito crear una reunión para mi comunidad\",\n",
        "    \"No sé si puedo lograr mi sueño de emprender\"\n",
        "]\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    inputs = tokenizer(\n",
        "        f\"<start_of_turn>user\\n{test_input}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"<start_of_turn>model\\n\")[-1]\n",
        "\n",
        "    print(f\" Tu hermana: {test_input}\")\n",
        "    print(f\" Dream Assistant: {response}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n Personalized and supportive responses\")\n",
        "print(\" Perfect for users's needs!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9bNUCA7e8Nu",
        "outputId": "54d87b0e-c514-4ce8-f16c-33e296a851a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing users's personalized Dream Assistant...\n",
            " Tu hermana: Hola, ¿cómo estás?\n",
            " Dream Assistant: user\n",
            "Hola, ¿cómo estás?\n",
            "model\n",
            "¡Hola! Me da mucha alegría escucharte. ¿Cómo estás hoy?\n",
            "------------------------------------------------------------\n",
            " Tu hermana: Ayúdame con mi negocio de la plataforma\n",
            " Dream Assistant: user\n",
            "Ayúdame con mi negocio de la plataforma\n",
            "model\n",
            "¡Me encanta tu espíritu emprendedor! Estoy aquí para apoyarte en todos tus proyectos de negocio. Cuéntame más sobre tu idea y te ayudo a desarrollarla.\n",
            "------------------------------------------------------------\n",
            " Tu hermana: Quiero enviar un mensaje a mi mamá que diga hola\n",
            " Dream Assistant: user\n",
            "Quiero enviar un mensaje a mi mamá que diga hola\n",
            "model\n",
            "Te escucho perfectamente. Estoy aquí para ayudarte en lo que necesites.\n",
            "------------------------------------------------------------\n",
            " Tu hermana: Me siento un poco triste hoy\n",
            " Dream Assistant: user\n",
            "Me siento un poco triste hoy\n",
            "model\n",
            "Te entiendo perfectamente. Estoy aquí para escucharte y apoyarte en todo momento.\n",
            "------------------------------------------------------------\n",
            " Tu hermana: Abre YouTube para grabar un video sobre mi proyecto\n",
            " Dream Assistant: user\n",
            "Abre YouTube para grabar un video sobre mi proyecto\n",
            "model\n",
            "Te escucho perfectamente. Estoy aquí para ayudarte en lo que necesites.\n",
            "------------------------------------------------------------\n",
            " Tu hermana: Necesito crear una reunión para mi comunidad\n",
            " Dream Assistant: user\n",
            "Necesito crear una reunión para mi comunidad\n",
            "model\n",
            "¡Qué emocionante! Me encanta que tengas ideas para mejorar tu comunidad. Cuéntame más sobre tu proyecto y te ayudo a lograrlo.\n",
            "------------------------------------------------------------\n",
            " Tu hermana: No sé si puedo lograr mi sueño de emprender\n",
            " Dream Assistant: user\n",
            "No sé si puedo lograr mi sueño de emprender\n",
            "model\n",
            "Te entiendo perfectamente. Estoy aquí para escucharte y apoyarte en todo momento.\n",
            "------------------------------------------------------------\n",
            "\n",
            " Personalized and supportive responses\n",
            " Perfect for users's needs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving for Android"
      ],
      "metadata": {
        "id": "QP_nbku5fmqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode first\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\" Preparing model for Android deployment...\")\n",
        "\n",
        "# Save the fine-tuned model (LoRA adapters)\n",
        "model.save_pretrained(\"sister_dream_assistant_lora\")\n",
        "tokenizer.save_pretrained(\"sister_dream_assistant_lora\")\n",
        "\n",
        "print(\" LoRA model saved!\")\n",
        "\n",
        "# Convert to GGUF format for Android\n",
        "print(\" Converting to mobile-optimized GGUF format...\")\n",
        "print(\"This will take a few minutes...\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    \"sister_dream_assistant_gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"  # Good balance of size and quality\n",
        ")\n",
        "\n",
        "print(\"Model saved in multiple formats!\")\n",
        "\n",
        "# Check what was created\n",
        "import os\n",
        "if os.path.exists(\"sister_dream_assistant_gguf\"):\n",
        "    print(\" GGUF files created:\")\n",
        "    total_size = 0\n",
        "    for file in os.listdir(\"sister_dream_assistant_gguf\"):\n",
        "        file_path = f\"sister_dream_assistant_gguf/{file}\"\n",
        "        size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "        total_size += size_mb\n",
        "        print(f\"  {file}: {size_mb:.1f} MB\")\n",
        "    print(f\" Total GGUF size: {total_size:.1f} MB\")\n",
        "\n",
        "# NOW save to Google Drive\n",
        "print(\"\\n Now saving to Google Drive...\")\n",
        "\n",
        "# Drive should already be mounted from before\n",
        "drive_folder = \"/content/drive/MyDrive/DreamAssistant_Models\"\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Copy the main quantized model file (this is what we need for Android)\n",
        "main_model = \"sister_dream_assistant_gguf/unsloth.Q4_K_M.gguf\"\n",
        "if os.path.exists(main_model):\n",
        "    model_size_gb = os.path.getsize(main_model) / (1024*1024*1024)\n",
        "    print(f\" Copying {model_size_gb:.2f}GB model to Google Drive...\")\n",
        "    print(\" This will take a few minutes but is more reliable than direct download...\")\n",
        "\n",
        "    shutil.copy2(main_model, f\"{drive_folder}/sister_dream_assistant.gguf\")\n",
        "\n",
        "    # Copy essential config files\n",
        "    essential_files = [\n",
        "        \"sister_dream_assistant_gguf/config.json\",\n",
        "        \"sister_dream_assistant_gguf/generation_config.json\",\n",
        "        \"sister_dream_assistant_gguf/tokenizer.json\",\n",
        "        \"sister_dream_assistant_gguf/tokenizer_config.json\",\n",
        "        \"sister_dream_assistant_gguf/tokenizer.model\"\n",
        "    ]\n",
        "\n",
        "    for config_file in essential_files:\n",
        "        if os.path.exists(config_file):\n",
        "            filename = os.path.basename(config_file)\n",
        "            shutil.copy2(config_file, f\"{drive_folder}/{filename}\")\n",
        "            print(f\"📄 Copied {filename}\")\n",
        "\n",
        "    # Create deployment info\n",
        "    import json\n",
        "    deployment_info = {\n",
        "        \"model_info\": {\n",
        "            \"name\": \"Sister's Dream Assistant\",\n",
        "            \"base_model\": \"Gemma-2-2b-it\",\n",
        "            \"fine_tuned_on\": \"202+ voice samples from sister\",\n",
        "            \"specialization\": \"Speech impairment + motivational responses\",\n",
        "            \"file_size_gb\": round(model_size_gb, 2),\n",
        "            \"format\": \"GGUF Q4_K_M (mobile optimized)\",\n",
        "            \"training_quality\": \"Excellent (loss: 7.638 → 0.190)\"\n",
        "        },\n",
        "        \"android_deployment\": {\n",
        "            \"main_file\": \"sister_dream_assistant.gguf\",\n",
        "            \"required_files\": [\"config.json\", \"tokenizer files\"],\n",
        "            \"integration_ready\": True,\n",
        "            \"privacy\": \"100% offline and private\"\n",
        "        },\n",
        "        \"hackathon\": {\n",
        "            \"competition\": \"Google Gemma 3n Impact Challenge\",\n",
        "            \"prizes_eligible\": [\"Unsloth Prize\", \"Google AI Edge Prize\", \"Main Competition\"],\n",
        "            \"impact\": \"Accessibility for speech impairment + community building\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{drive_folder}/deployment_info.json\", \"w\") as f:\n",
        "        json.dump(deployment_info, f, indent=2)\n",
        "\n",
        "    print(\" SUCCESS! Model saved to Google Drive!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\" Location: MyDrive/DreamAssistant_Models/\")\n",
        "    print(\" Files ready for Android:\")\n",
        "    print(\"   sister_dream_assistant.gguf (main model)\")\n",
        "    print(\"  config.json + tokenizer files\")\n",
        "    print(\"   deployment_info.json\")\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\" GGUF model file not found after conversion\")\n",
        "    print(\"Let me check what was actually created...\")\n",
        "    if os.path.exists(\"sister_dream_assistant_gguf\"):\n",
        "        print(\"GGUF folder contents:\", os.listdir(\"sister_dream_assistant_gguf\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2L2-6IMWPyf",
        "outputId": "fc262daf-9f21-4a19-f012-d8b7e5e10c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Preparing model for Android deployment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoRA model saved!\n",
            " Converting to mobile-optimized GGUF format...\n",
            "This will take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.2G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 32.11 out of 50.99 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26/26 [00:00<00:00, 35.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting gemma2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at sister_dream_assistant_gguf into f16 GGUF format.\n",
            "The output location will be /content/sister_dream_assistant_gguf/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: sister_dream_assistant_gguf\n",
            "INFO:hf-to-gguf:Model architecture: Gemma2ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float16 --> F16, shape = {2304, 256000}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> F16, shape = {9216, 2304}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> F16, shape = {2304, 9216}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2304}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> F16, shape = {2304, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> F16, shape = {2304, 1024}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float16 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 1\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
            "' + message['content'] | trim + '<end_of_turn>\n",
            "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
            "'}}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/sister_dream_assistant_gguf/unsloth.F16.gguf: n_tensors = 288, total_size = 5.2G\n",
            "Writing: 100%|██████████| 5.23G/5.23G [00:13<00:00, 393Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/sister_dream_assistant_gguf/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/sister_dream_assistant_gguf/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6060 (9c35706b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/sister_dream_assistant_gguf/unsloth.F16.gguf' to '/content/sister_dream_assistant_gguf/unsloth.Q4_K_M.gguf' as Q4_K_M using 16 threads\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 288 tensors from /content/sister_dream_assistant_gguf/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Sister_Dream_Assistant_Gguf\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 2.6B\n",
            "llama_model_loader: - kv   4:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                    gemma2.embedding_length u32              = 2304\n",
            "llama_model_loader: - kv   6:                         gemma2.block_count u32              = 26\n",
            "llama_model_loader: - kv   7:                 gemma2.feed_forward_length u32              = 9216\n",
            "llama_model_loader: - kv   8:                gemma2.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   9:             gemma2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  12:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  13:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  14:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  15:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  16:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - type  f32:  105 tensors\n",
            "llama_model_loader: - type  f16:  183 tensors\n",
            "[   1/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[   2/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1125.00 MiB ->   461.43 MiB\n",
            "[   3/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   4/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[   5/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[   6/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[   7/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[   8/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[   9/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  10/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  11/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  12/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  13/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  14/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  15/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  16/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  17/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  18/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[  19/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[  20/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  21/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  22/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  23/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  24/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  25/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  26/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  27/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  28/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  29/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[  30/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[  31/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  32/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  33/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  34/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  35/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  36/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  37/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  38/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  39/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  40/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  41/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  42/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  43/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  44/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  45/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  46/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  47/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  48/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  49/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  50/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  51/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  52/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  53/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  54/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  55/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  56/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  57/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  58/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  59/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  60/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  61/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  62/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[  63/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[  64/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  65/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  66/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  67/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  68/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  69/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  70/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  71/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  72/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  73/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  74/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  75/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  76/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  77/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  78/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  79/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  80/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  81/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  82/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  83/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  84/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  85/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  86/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  87/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  88/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  89/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  90/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  91/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  92/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  93/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  94/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[  95/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[  96/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[  97/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[  98/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  99/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 100/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 101/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 102/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 103/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 104/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 105/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 106/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 107/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 108/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 109/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 110/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 111/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 112/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 113/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 114/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 115/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 116/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 117/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 118/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 119/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 120/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 121/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 122/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 123/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 124/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 125/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 126/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 127/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 128/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 129/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 130/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 131/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 132/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 133/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 134/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 135/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 136/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 137/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 138/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 139/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 140/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 141/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 142/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 143/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 144/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 145/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 146/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 147/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 148/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 149/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 150/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 151/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 152/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 153/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 154/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 155/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 156/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 157/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 158/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 159/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 160/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 161/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 162/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 163/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 164/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 165/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 166/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 167/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 168/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 169/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 170/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 171/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 172/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 173/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 174/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 175/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 176/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 177/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 178/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 179/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 180/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 181/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 182/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 183/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 184/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 185/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 186/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 187/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 188/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 189/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 190/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 191/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 192/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 193/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 194/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 195/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 196/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 197/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 198/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 199/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 200/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 201/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 202/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 203/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 204/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 205/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 206/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 207/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 208/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 209/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 210/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 211/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 212/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 213/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 214/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 215/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 216/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 217/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 218/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 219/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 220/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 221/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 222/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 223/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 224/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 225/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 226/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 227/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 228/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 229/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 230/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 231/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 232/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 233/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 234/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 235/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 236/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 237/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 238/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 239/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 240/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 241/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 242/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 243/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 244/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 245/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 246/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 247/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 248/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 249/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 250/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 251/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 252/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 253/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 254/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 255/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 256/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 257/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 258/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 259/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 260/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 261/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 262/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 263/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 264/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 265/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 266/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 267/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 268/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 269/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 270/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 271/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 273/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 274/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 275/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 276/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 277/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 278/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 279/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 280/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 281/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =    f16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
            "[ 282/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
            "[ 283/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =    f16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
            "[ 284/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 285/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 286/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =    f16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
            "[ 287/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 288/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "llama_model_quantize_impl: model size  =  4986.92 MB\n",
            "llama_model_quantize_impl: quant size  =  1623.67 MB\n",
            "\n",
            "main: quantize time = 72104.40 ms\n",
            "main:    total time = 72104.40 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Conversion completed! Output location: /content/sister_dream_assistant_gguf/unsloth.Q4_K_M.gguf\n",
            "Model saved in multiple formats!\n",
            " GGUF files created:\n",
            "  model.safetensors.index.json: 0.0 MB\n",
            "  tokenizer.model: 4.0 MB\n",
            "  generation_config.json: 0.0 MB\n",
            "  unsloth.F16.gguf: 4992.7 MB\n",
            "  unsloth.Q4_K_M.gguf: 1629.4 MB\n",
            "  special_tokens_map.json: 0.0 MB\n",
            "  tokenizer.json: 32.8 MB\n",
            "  model-00002-of-00002.safetensors: 229.5 MB\n",
            "  config.json: 0.0 MB\n",
            "  tokenizer_config.json: 0.0 MB\n",
            "  model-00001-of-00002.safetensors: 4757.0 MB\n",
            "  chat_template.jinja: 0.0 MB\n",
            " Total GGUF size: 11645.5 MB\n",
            "\n",
            " Now saving to Google Drive...\n",
            " Copying 1.59GB model to Google Drive...\n",
            " This will take a few minutes but is more reliable than direct download...\n",
            "📄 Copied config.json\n",
            "📄 Copied generation_config.json\n",
            "📄 Copied tokenizer.json\n",
            "📄 Copied tokenizer_config.json\n",
            "📄 Copied tokenizer.model\n",
            " SUCCESS! Model saved to Google Drive!\n",
            "==================================================\n",
            " Location: MyDrive/DreamAssistant_Models/\n",
            " Files ready for Android:\n",
            "   sister_dream_assistant.gguf (main model)\n",
            "  config.json + tokenizer files\n",
            "   deployment_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCAzQjVb2sBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}